---
title: "Simple Prediction Algorithm"
output:
  html_document:
    theme: united
    toc: true
    toc_float: true
  pdf_document: default
date: "2024-10-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,warning=FALSE,echo=FALSE,include=FALSE}
library(ggplot2)
library(quantmod)
library(roll)
library(lmtest)
library(tidyr)
library(readxl)
library(kableExtra)

apple_df <- getSymbols('AAPL',src='yahoo',auto.assign = F)
```


# Problem Statement and Motivation

When studying optimization methods like gradient descent, one might find it intreaging to tackle a relevant question. As someone who enjoys predictive modeling, the stock market is a natural place for me to go to try my new data science skills. This brings us to the problem statement. Is there a simple stock prediction algorithm that is easy to implement, but allows us to use gradient descent and confidence intervals?

# Methodology

To implement optimization algorithms to predict, we will observe stock prices and the opportunity for simple models to show us how simple prediction can be.

Important Aspects of this Project:</br>

1. Gradient Descent</br>
2. Confidence Intervals</br>

We will use the following function for the predicted price:
$$p(t+1)=\mu(t)+\sigma(t)$$

Where: </br>
$\mu(t)=$ moving average</br>
$\sigma(t)=$moving standard deviation</br>

```{r}
# Define the Prediction Function
pred.price <- function(moving.avg,moving.sd,a = 1){
  return(moving.avg + a * moving.sd)
}
```

When using gradient descent, it is important to define a loss function. In our case, we will use the mean sum squares function.

$$MSE = \frac{1}{n}\sum^{n}_{i=1}(\mu_{t}-x_{pred})^2$$

```{r}
# Loss Function:

# We will use the sum squares error:
loss.function <- function(a,prices,moving.avg,moving.standard.d){
  #error <- prices - moving.avg
  
  # Predict future Prices
  pred.prices <- moving.avg + moving.standard.d
  
  # Calculate the sum of squared errors (SSE)
  sse <- sum((prices - pred.prices)^2)
  
  return(sse)
}
```

Now, let us upload the stock data and implement our algorithm using a three-day moving average and a three-day moving standard deviation:

```{r, warning=FALSE}
left <- 1000
right <- 1150
prices <- apple_df$AAPL.Close[left:right]


simple.algo <<- function(data,omega,min,max){
  prices <- data[min:max]
  # 3-day MA
  moving.average <- stats::filter(prices,rep(1/3, 3), sides = 1)
  # 3-day MSD
  moving.standard.d <- roll_sd(prices,3)
  # 3-day MA + 3-day MSD
  func <- moving.average + omega * moving.standard.d 
  # error
  error <- prices - func
  
  # Remove NA values
  prices <- prices[!is.na(moving.average)]
  moving.average <- moving.average[!is.na(moving.average)]
  moving.standard.d <- moving.standard.d[!is.na(moving.standard.d)]
  func <- func[!is.na(func)]
  error <- round(error[!is.na(error)],3)
  

  #Let us Make a new dataframe with all of the values:
  new.df <- as.data.frame(cbind(prices,
                              moving.average,
                              moving.standard.d,
                              func,
                              error))
                          
  colnames(new.df) <- c("Prices",
                        "MovingAverage",
                        "MovingStandardDev",
                        "Func",
                        "Error")
  
  return(new.df)
}

origional <- simple.algo(prices,1,0,length(prices))
```


Let's observe the error
```{r, echo=FALSE}
cat("Error Mean: ",mean(origional$Error),"\n")
cat("Error Standard Deviation: ",sd(origional$Error),"\n")
cat("Mean Squares of Error: ", mean(origional$Error^2))
```

# Gradient Descent

Let us add a coefficient ($\omega$) to our model and use gradient descent to optimize it. Our new model is:


$$price = \mu(t)+\omega \sigma(t)$$
Explanation of Gradient Descent: Link

Let us use gradient descent to optimize this coefficient:

Gradient Descent Code:
```{r}
gradient.descent <- function(data,learning.rate=0.01,iterations=1000){
  # Naming Variables
  price <- data$Price
  moving.average <- data$MovingAverage
  moving.standard.d <- data$MovingStandardDev
  
  # Number of Training Examples:
  m <- length(price)
  
  # Initiate omega
  omega <- 0
  
  omega.values <- numeric(iterations)
  
  # Gradient Descent Loop:
  for(i in 1:iterations){
    #print(omega)
    # Compute the Predictions Based on Current Data:
    predictions <- moving.average + omega * moving.standard.d
    
    # Calculate the Error:
    error <- predictions - price
    
    # Compute Gradient:
    gradient <- (1/m) * sum(error * moving.standard.d)
    #cat("Gradient: ",gradient)
    
    # Update Omega
    omega <- omega - learning.rate * gradient
    
    omega.values[i] <- omega
  }
  # Return Optimized Values:
  return(omega)
}
```

Let us implement Gradient Descent, we will output the coefficient $\omega$ and the time it takes for the algorithm to complete:
```{r}
# GD Function
time.a <- proc.time()[3]
omega <- gradient.descent(origional,0.01,100)
time.b <- proc.time()[3]

# Time and Coefficient:
cat("Time to Complete: ",time.b - time.a,"\n",
    "Coefficient: ",omega)
```

Let us use our new omega with our algorithm:
```{r, warning=FALSE}
# New Model
optim.df <- simple.algo(prices,omega,0,length(prices))
```

MSE for the Original and Optimized Model:
```{r, echo=FALSE}
# Calculate MSE
cat("Origional Model MSE: ",mean(origional$Error^2),"\n",
    "Optimized Model MSE: ",mean(optim.df$Error^2))
```

# Iterative Gradient Descent

Let us see if we can get a better MSE by using an iterative gradient descent method where we update beta in an iterative manner, therefore, instead of one $\omega$, we will have a vector of $\omega$ that could prove to minimize error further.

Iterative Gradient Descent Method with an update after every 3 iterations:
```{r}
# This will return a list of omega values for each iteration of our model:
iterative.gradient.descent <- function(data,learning.rate=0.01,iterations=1000){
  # Naming Variables
  price <- data$Price
  moving.average <- data$MovingAverage
  moving.standard.d <- data$MovingStandardDev
  
  # Omega Values:
  omega.vals <- numeric(length(price))
  
  # Initiate omega (parameters) as a Vector of Zeros
  omega <- 0
  
  # Gradient Descent Loop:
  for(t in 1:length(price)){
    #print(omega)
    # Compute the Predictions Based on Current Data:
    predictions <- moving.average[t] + omega * moving.standard.d[t]
    
    # Calculate the Error:
    error <- predictions - price[t]
    
    # Compute Gradient:
    gradient <- error * moving.standard.d
    #cat("Gradient: ",gradient)
    
    # Update Omega every n iterations:
    n = 3
    if(t %% n == 0){
      omega <- omega - learning.rate * gradient
    }else{
      omega <- omega
    }
    
    
  }
  # Return Optimized Values:
  return(omega)
}
```

Implement Iterative Gradient Descent on our Data and observe the first six values of our $\omega$ vector:
```{r}
# GD
time.a <- proc.time()[3]
iter.omega <- iterative.gradient.descent(origional)
time.b <- proc.time()[3]
it.time <- time.b - time.a

# Observe Omega Values
head(iter.omega)

# Final Omega Value:
f.omega <- iter.omega[length(iter.omega)]
```

Asses the Models:
```{r, warning=FALSE,echo=FALSE}
# Iterative Model:
iter.model2 <- simple.algo(prices,iter.omega,0,length(prices))

# Calculate MSE
cat("Origional Model MSE: ",mean(origional$Error^2),"\n",
    "Optimized Model MSE: ",mean(optim.df$Error^2),"\n",
    "Iterative Optimized Model MSE: ",mean(iter.model2$Error^2),"\n")
```
**Note:** The iterative optimized model is programmed to change the parameter $\omega$ after 3 iterations.</br>

The model with the smallest MSE is the optimized model. Therefore, we will do more analysis on this model.

# Error Analysis

Let us quickly do an analysis on our error distribution of the Optimized Model:


```{r, warning=FALSE,echo=FALSE}
plot(optim.df$Error,pch=16,xlab = "Index",ylab = "Error",main = "Optimized Model Error Scatter Plot")
```


```{r, echo=FALSE}
qqnorm(iter.model2$Error,)
qqline(iter.model2$Error)
```

We can also implement a Shapiro-Wilk test for normality:
```{r}
shapiro.test(optim.df$Error)
```

We can see from the q-q plot and the Shapiro-Wilk test, that the error is not normally distributed. In order to find confidence intervals, we will bootstrap to find the mean and standard deviation. We will use the "boot" package.


# Confidence Interval

```{r, warning=FALSE}
library(boot)
error <- iter.model2$Error

# Define a function for the statistic of interest:
mean.stat <- function(data,indices){
  d <- data[indices]
  return(mean(abs(d))) # Return Mean Absolute Error
}

# Bootstrap Loop:
bootstrap.results <- boot(error,statistic = mean.stat,R = 1000)
cat("Bootstrap Mean: ",mean(bootstrap.results$t),"\n","Sample Mean",mean(error),"\n")
cat()

# Calculate CI:
ci <-boot.ci(bootstrap.results, type = "perc", conf = 0.95) # Using Percentile Method
ci
```


# Confidence Error Visualization

**Note: We will use absolute error since our confidence interval is in absolute error.**
```{r}
# Retrieve Bootstrap Mean and Confidence Intervals:
ci.lower <- ci$percent[4] # 2.5th Percentile for 95% ci
ci.upper <- ci$percent[5] # 97.5th percentile for 95% ci
bootstrap.mean <- mean(bootstrap.results$t)
data <- abs(error)

# Plot the Data and Confidence Band:
plot(data, main = "Data with Bootstrap Confidence Interval for the Mean", 
     ylab = "Value", xlab = "Index", col = "blue", pch = 16, ylim = c(min(data) - 5, max(data) + 5))
abline(h = bootstrap.mean, col = "red", lwd = 2, lty = 2)  # Bootstrapped mean line
abline(h = ci.lower, col = "darkgreen", lwd = 2, lty = 2)  # Lower CI
abline(h = ci.upper, col = "darkgreen", lwd = 2, lty = 2)  # Upper CI
polygon(c(1, length(data), length(data), 1), c(ci.lower, ci.lower, ci.upper, ci.upper), 
        col = rgb(0.1, 0.9, 0.1, 0.2), border = NA)  # Shaded confidence band
legend("topright", legend = c("Bootstrap Mean", "95% CI"), col = c("red", "darkgreen"), 
       lty = 2, lwd = 2, bty = "n")
```

# Prediction interval for price

Quick Background on Prediction Intervals:

Note: We do not need to know the distribution of the prices for the prediction interval, merely the distribution of the error. By bootstrapping the standard deviation, we are effectively estimating the uncertainty around the predictions without making strong assumptions about the underlying distribution of the prices.

Steps to Construct a Prediction Interval:</br>
</br>
1. Calculate Predicted Prices</br>
2. Bootstrap the Standard Deviation of Prediction Errors</br>
3. Define Critical Value: We will use a t-distribution of the sample size is small, normal (z) if the sample size is large enough (>30)</br>
4. Construct a Prediction Interval</br>

Prediction Interval Formula:</br>

Prediction Interval = Prediction Price $\pm$ t $\cdot$ Standard Deviation of Prediction Error

Mathematically:
$$[\hat{y}_{low},\hat{y}_{high}]=f(t)_{pred}\text{ }\pm\text{ }z_{\frac{\alpha}{2}}\cdot\sigma_{pred}$$

```{r}
# We will use the best model's error to bootstrap: Optimized Model
error <- optim.df$Error

# Define a function for the statistic of interest:
sd.stat <- function(data,indices){
  d <- data[indices]
  return(sd(d)) # Return Mean Absolute Error
}

# Bootstrap Loop:
bootstrap.sd <- boot(error,statistic = sd.stat,R = 1000)
bs.sd <- sd(bootstrap.sd$t)


# Critical Value for 95% Confidence Interval
alpha <- 0.05
z <- qnorm(1 - alpha/2)

# Construct Confidence Interval for Predicted Prices
lower.ci <- optim.df$Func - z * bs.sd
upper.ci <- optim.df$Func + z * bs.sd

# Display Results:
pred.int <- data.frame(origional$Func,optim.df$Price,optim.df$Func,lower.ci,upper.ci)

cat("Bootstrap Standard Deviation: ",bs.sd,"\n",
    "Sample Standard Deviation",sd(error),"\n")

head(pred.int)
```

Plot Prediction Interval:
```{r, warning=FALSE}
# Load necessary library
library(ggplot2)

# Assuming `predicted`, `lower_pi`, and `upper_pi` are vectors with the same length
# and you have a data frame `data` containing the actual prices (for context)

# Create a data frame for plotting
plot_data <- data.frame(
  Actual = pred.int$optim.df.Price,         # Actual prices
  Original = pred.int$origional.Func, # Original Model
  Predicted = pred.int$optim.df.Func,   # Predicted prices from your model
  Lower_PI = pred.int$lower.ci,     # Lower bound of the prediction interval
  Upper_PI = pred.int$upper.ci      # Upper bound of the prediction interval
)

# Plot
ggplot(plot_data, aes(x = 1:nrow(plot_data))) + 
  geom_line(aes(y = Actual), color = "red", size = 1, linetype = "dashed") +  # Actual prices
  geom_line(aes(y = Predicted), color = "blue", size = 1) +
  #geom_line(aes(y = Original), color = "gray", size = 1)+ # Original Model
  # Predicted prices
  geom_ribbon(aes(ymin = Lower_PI, ymax = Upper_PI), fill = "lightgrey", alpha = 0.5) +  # Prediction interval
  labs(title = "Prediction Interval for Stock Prices",
       x = "Time",
       y = "Price") +
  theme_minimal() +
  scale_x_continuous(breaks = seq(1, nrow(plot_data), by = 20)) +  # Adjust x-axis for better visibility
  theme(legend.position = "none")

```


# Final Analysis and Thoughts

<mark>**Note: The tables below are not dynamic. The values below are for the data set window $[3800:3900]$ to recreate this table, you must use the code to retrieve the $\omega$ and MSE values.**</mark>

After further analysis, the iterative model sometimes gives better MSE values than the optimized model depending on the sample size and the iterations. We will use a table to show our results of the analysis:

```{r, echo=FALSE}
# Import Table:
#-----------------------------------------------------------------
#-------- Use the spreadsheet in the GitHub Repository ---------
# ----------------------------------------------------------------

OmegaMSE <- OmegaMSE_Spreadsheet <- read_excel("C:/Users/EJ's/Desktop/GitHub/Mini Projects/Simple Slock Algorithm/Files/OmegaMSE Spreadsheet.xlsx")
View(OmegaMSE_Spreadsheet) # <-------- Change This!!!!

# Category Vector:
cat.vec <- c("Origional Model","Optimized Model","Iterative Model: n=3","Iterative Model: n=5")#,"Iterative Model: n=10","Iterative Model: n=20","Iterative Model: n=s/2")
sample.vec <- c("50","500","1000","Full Dataset")

# Make Full Omega and MSE Vectors:
omega.vec <- c(c(OmegaMSE$Omega.50),c(OmegaMSE$Omega.500),c(OmegaMSE$Omega.1000),c(OmegaMSE$Omega.Full))
mse.vec <- c(c(OmegaMSE$MSE.50),c(OmegaMSE$MSE.500),c(OmegaMSE$MSE.1000),c(OmegaMSE$MSE.Full))

# MSE Factor
mse.fac <- c("50","100","1000","Full")

# Sort Omega and MSE for Categories:
# Omega:
fifty.omega <- omega.vec[c(1:4)]
fiveHundred.omega <- omega.vec[c(8:11)]
oneThousand.omega <- omega.vec[c(15:18)]
fullData.omega <- omega.vec[c(22:25)]

# MSE:
fifty.mse <- mse.vec[c(1:4)] # Use 7 for full df
fiveHundred.mse <- mse.vec[c(8:11)] # Use 14 for full df
oneThousand.mse <- mse.vec[c(15:18)] # Use 21 for full df
fullData.mse <- mse.vec[c(22:25)] # Use 28 for full df

# Create a Full Data Frame:
full.data1 <- data.frame(cat.vec,fifty.omega,fifty.mse,
                        fiveHundred.omega,fiveHundred.mse,
                        oneThousand.omega,oneThousand.mse,
                        fullData.omega,fullData.mse)

# Create a Full Data Frame for Col Names:
full.data2 <- data.frame(cat.vec,fifty.omega,fifty.mse,
                        fiveHundred.omega,fiveHundred.mse,
                        oneThousand.omega,oneThousand.mse,
                        fullData.omega,fullData.mse)

colnames(full.data2) <- c("Model", "$\\omega$","$\\text{MSE}$",
                                  "$\\omega$","$\\text{MSE}$",
                                  "$\\omega$","$\\text{MSE}$",
                                  "$\\omega$","$\\text{MSE}$")

```

Model Equation:</br>
price = $\mu(t)+\omega\cdot\sigma(t)$
```{r, echo=FALSE}
knitr::kable(full.data2, caption = "Model Performance Metrics") %>%
  add_header_above(c(" " = 1, "Sample Size = 50" = 2, "Sample Size = 500" = 2, "Sample Size = 1000" = 2,"Sample Size = Full Data Set" = 2)) %>%
  kable_styling(full_width = F, bootstrap_options = c("striped","hover","condensed"))
```

```{r, echo=FALSE}
# Setting up Vectors
ss <- c("50","500","1000","Full Data")
model <- c(full.data1$cat.vec[which.min(full.data1$fifty.mse)],
           full.data1$cat.vec[which.min(full.data1$fiveHundred.mse)],
           full.data1$cat.vec[which.min(full.data1$oneThousand.mse)],
           full.data1$cat.vec[which.min(full.data1$fullData.mse)])
MSE <- c(min(fifty.mse),min(fiveHundred.mse),min(oneThousand.mse),min(fullData.mse))

# Dataframe:
table <- data.frame(ss,model,MSE)

# Column Names:
colnames(table) <- c("Sample Size","Best Model","MSE")

# Table:
knitr::kable(table, caption = "Best Models Per Sample Size") %>%
  kable_styling(full_width = F, bootstrap_options = c("striped","hover","condensed"))
```

This project is great as an introduction to gradient descent and confidence intervals.

